# Llamabox <img src="assets/icon.png" width="16" height="16" style="vertical-align: middle"> Llamabox â€” Offline AI on a Low-End PC

**Self-host AI assistants, RAG pipelines, and knowledge graphs on WSL2.**  
**No GPU. No OpenAI keys. Just your CPU.**

[![Distro Health](https://github.com/rajatasusual/llamabox/actions/workflows/check.yml/badge.svg)](https://github.com/rajatasusual/llamabox/actions/workflows/check.yml)  
![WSL2](https://img.shields.io/badge/WSL2-Supported-blue) ![Debian](https://img.shields.io/badge/Debian-Supported-blue) ![License](https://img.shields.io/badge/License-MIT-green)

---

## âœ… What You Get

- **`llama.cpp`** â†’ Fast CPU-only inference  
- **Redis Stack** â†’ Store and search vector embeddings  
- **Neo4j** â†’ Graph database for knowledge modeling  
- **Secure & resilient** â†’ All services auto-managed with `systemd`  
- **Browser extension** â†’ Capture web pages into your local knowledge base

âš¡ Runs on: **4GB RAM**, **no GPU**, **offline**, **WSL2 Debian**

---

## ğŸ–¥ï¸ Quick Start

```bash
# In Windows Terminal:
wsl --install -d Debian

# Then inside Debian:
sudo apt update && sudo apt install git -y
git clone https://github.com/rajatasusual/llamabox
cd llamabox
./setup.sh
```

ğŸ“˜ Full install guide: [INSTALLATION.md](docs/INSTALLATION.md)

---

## ğŸ› ï¸ Basic Usage

```bash
# Check services
./scripts/check.sh

# Logs for AI server
sudo journalctl -u llama-server.service
```

ğŸ“˜ Manage services: [MANAGE.md](docs/MANAGE.md)

---

## ğŸ§  RAG Pipeline Flow

```
[User Query] â†’ llama.cpp â†’ Redis (vectors) â†’ Neo4j (facts) â†’ Response
```

All local. All CPU. No cloud needed.

---

## â“ Common Fixes

> See full FAQ: [FAQs.md](docs/FAQs.md)

- ğŸ”§ **Systemd not working?**  
  Add this to `/etc/wsl.conf`:
  ```ini
  [boot]
  systemd=true
  ```

- ğŸ§  **Model OOM crash?**  
  Try a smaller `.gguf` model or bump RAM via `.wslconfig`.

---

## ğŸ”— Links

- ğŸ“š [Full Docs](docs/SUMMARY.md)  
- ğŸŒ [Browser Extension](https://github.com/rajatasusual/llamabox_extension)  
- ğŸ“ˆ [Benchmarks](#performance-benchmarks)

---

## ğŸ‘¥ Contribute

PRs welcome â€” fork â†’ branch â†’ PR  
Feature requests? Issues always appreciated.

---

## ğŸ“œ License

MIT. Built with â¤ï¸ using:
- [llama.cpp](https://github.com/ggml-org/llama.cpp)  
- [Redis](https://redis.io)  
- [Neo4j](https://neo4j.com)  
- WSL2 (because Linux on Windows rocks)