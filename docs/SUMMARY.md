### **Llamabox - Setup Summary**  

This is a **high-level overview** of your full setupâ€”spanning installation, AI services, database integrations, security, and performance optimization.  

For command-line references, see [MANAGE.md](/docs/MANAGE.md).

---

## ðŸ›  1. System Setup (WSL & Debian)  
âœ… Install **WSL2** and **Debian** on Windows  
âœ… Allocate **at least 4GB RAM & 20GB storage**  
âœ… Clone the repo and run the auto-installer  

```bash
wsl --install -d Debian
git clone https://github.com/rajatasusual/llamabox.git
cd llamabox
./setup.sh
```

> ðŸ’¡ For WSL2 IP, memory, and shutdown commands, see `MANAGE.md > PowerShell`.

---

## ðŸ“¦ 2. Database & Vector Store  

### ðŸ”´ **Redis Stack (Vector Embeddings)**  
âœ… Installed and configured **Redis Stack**  
âœ… Enabled **persistence** to avoid data loss  
âœ… Auto-start on boot with `systemctl enable redis`  

```bash
sudo systemctl start redis
```

---

### ðŸŸ¢ **Neo4j (Graph Storage)**  
âœ… Installed and configured **Neo4j**  
âœ… Opened to host with `server.default_listen_address=0.0.0.0`  
âœ… Auto-restarts on failure via `systemd`  

```bash
sudo nano /etc/neo4j/neo4j.conf   # Set listen address to 0.0.0.0
sudo systemctl restart neo4j
```

---

## ðŸ§  3. AI Model Inference (llama.cpp)  

### ðŸ¤– **llama.cpp (CPU-only Inference)**  
âœ… Built from source with local model integration  
âœ… Benchmarked **3B model** at **~253 tokens/sec**  
âœ… Exposed `llama-server` on all interfaces (`0.0.0.0`)  
âœ… Works with Redis for **RAG-style document Q&A**  

```bash
llama-server --host 0.0.0.0
```

---

## ðŸ›¡ 4. Security & Resilience  

âœ… Hardened the WSL2 Debian environment:  
- Disabled **root SSH login**  
- Enabled **firewall (UFW)** and **Fail2Ban**  
- Enabled **unattended security upgrades**  

âœ… Ensured service resilience:  
- All key services auto-restart via `systemd`  
- Redis and Neo4j recover from crash or reboot  

```bash
sudo systemctl enable redis neo4j
```

> ðŸ” See `MANAGE.md > Service Management` for logs and status checks.

---

## ðŸš€ 5. Running the Full Stack  

Use the all-in-one checker to ensure all services are up:

```bash
./scripts/check.sh
```

This will:  
âœ… Start `redis`, `neo4j`, `llama-server`, `embed-server`, `redis-worker`, and `http-server`  
âœ… Monitor & restart them automatically  
âœ… Enable access from your Windows browser or tools  

---

## ðŸ“Š 6. Performance Benchmarks  

### ðŸ§ª Test Environment  
- **CPU**: AMD Z1 Extreme (4 Cores)  
- **RAM**: 4GB  
- **Disk**: 20GB SSD  
- **OS**: Debian on WSL2  
- **GPU**: None (CPU-only inference)  

### ðŸ“ˆ Model Performance  

| Model                 | Tokens/sec | RAM Usage |
|----------------------|------------|-----------|
| **LLaMA 3B Q8_0**     | **253 t/s** | ~900MB    |
| **Mistral 7B Q4_K_S** | ~6 t/s     | ~4GB      |
| **LLaMA 13B Q8_0**    | ~2 t/s     | ~10GB     |

> ðŸ§  Performance will vary based on WSL config and CPU throttling.  

---

## ðŸ”œ Next Steps  

âœ… Run queries through Redis-backed RAG pipeline  
âœ… Extend with **LangChain workflows** or **custom embedding sources**  
âœ… Use [MANAGE.md](./MANAGE.md) to debug, monitor, and tune system performance  

---

This setup is now **fully optimized for low-end devices** running **CPU-only inference**.